##### 参考

```
https://www.cnblogs.com/bakari/p/10564347.html

字符驱动
https://zhuanlan.zhihu.com/p/556031063
https://segmentfault.com/a/1190000009249039 (重点)


linux 目录
https://blog.csdn.net/shulianghan/article/details/123718138


https://cloud.tencent.com/developer/article/2026737
https://mp.weixin.qq.com/s?__biz=Mzk0ODI3NDQ0NQ==&mid=2247483988&idx=1&sn=d1d06748eea509e2da4882cff5027595&chksm=c36b5636f41cdf2065bdcc2cd3eedcc4adeac4cb7915a68e8d3b2f6190d47582d69481dc3aee&scene=21#wechat_redirect


tun 驱动位于 drivers/net/tun.c


https://mp.weixin.qq.com/s?__biz=Mzk0ODI3NDQ0NQ==&mid=2247483965&idx=2&sn=68a9efcf98fbdf2d7eb962306f3d0c89&chksm=c36b565ff41cdf490c0d4df28ff2101d07fe862515e9f6dd8bea361932cbf9e9502a08e553f1&scene=21#wechat_redirect
```

# 网络数据包接收过程

> 以一个 UDP 包的接收过程作为示例

##### 参考

```
重点看和下面的评论
https://segmentfault.com/a/1190000008836467
https://segmentfault.com/a/1190000008926093
```

##### 数据包网卡到内存

```
                   +-----+
                   |     |                            Memroy
+--------+   1     |     |  2  DMA     +--------+--------+--------+--------+
| Packet |-------->| NIC |------------>| Packet | Packet | Packet | ...... |
+--------+         |     |             +--------+--------+--------+--------+
                   |     |<--------+
                   +-----+         |
                      |            +---------------+
                      |                            |
                    3 | Raise IRQ                  | Disable IRQ
                      |                          5 |
                      |                            |
                      ↓                            |
                   +-----+                   +------------+
                   |     |  Run IRQ handler  |            |
                   | CPU |------------------>| NIC Driver |
                   |     |       4           |            |
                   +-----+                   +------------+
                                                   |
                                                6  | Raise soft IRQ
                                                   |
                                                   ↓
```

```
1. 数据包 (Packet) 从外面的网络进入物理网卡。如果目的地址不是该网卡, 且该网卡没有开启混杂模式, 该包会被网卡丢弃。
2. 网卡将数据包通过 DMA 的方式写入到指定的内存地址, 该地址由网卡驱动分配并初始化(老的网卡可能不支持DMA, 不过新的网卡一般都支持)
3. 网卡通过硬件中断（IRQ）通知CPU, 告诉它有数据来了
4. CPU根据中断表, 调用已经注册的中断函数, 这个中断函数会调到驱动程序（NIC Driver）中相应的函数
5. 驱动先禁用网卡的中断, 表示驱动程序已经知道内存中有数据了, 告诉网卡下次再收到数据包直接写内存就可以了, 不要再通知CPU了, 这样可以提高效率, 避免CPU不停的被中断。
6. 启动软中断。这步结束后, 硬件中断处理函数就结束返回了。由于硬中断处理程序执行的过程中不能被中断, 所以如果它执行时间过长, 会导致CPU没法响应其它硬件的中断, 于是内核引入软中断, 这样可以将硬中断处理函数中耗时的部分移到软中断处理函数里面来慢慢处理。
```

##### 内核的网络模块

```
                                                     +-----+
                                             17      |     |
                                        +----------->| NIC |
                                        |            |     |
                                        |Enable IRQ  +-----+
                                        |
                                        |
                                  +------------+                                      Memroy
                                  |            |        Read           +--------+--------+--------+--------+
                 +--------------->| NIC Driver |<--------------------- | Packet | Packet | Packet | ...... |
                 |                |            |          9            +--------+--------+--------+--------+
                 |                +------------+
                 |                      |    |        skb
            Poll | 8      Raise softIRQ | 6  +-----------------+
                 |                      |             10       |
                 |                      ↓                      ↓
         +---------------+  Call  +-----------+        +------------------+        +--------------------+  12  +---------------------+
         | net_rx_action |<-------| ksoftirqd |        | napi_gro_receive |------->| enqueue_to_backlog |----->| CPU input_pkt_queue |
         +---------------+   7    +-----------+        +------------------+   11   +--------------------+      +---------------------+
                                                               |                                                      | 13
                                                            14 |        + - - - - - - - - - - - - - - - - - - - - - - +
                                                               ↓        ↓
                                                    +--------------------------+    15      +------------------------+
                                                    | __netif_receive_skb_core |----------->| packet taps(AF_PACKET) |
                                                    +--------------------------+            +------------------------+
                                                               |
                                                               | 16
                                                               ↓
                                                      +-----------------+
                                                      | protocol layers |
                                                      +-----------------+
```

```
7. 内核中的 ksoftirqd 进程专门负责软中断的处理, 当它收到软中断后, 就会调用相应软中断所对应的处理函数, 对于上面第6步中是网卡驱动模块抛出的软中断, ksoftirqd 会调用网络模块的 net_rx_action 函数
8. net_rx_action 调用网卡驱动里的 poll 函数来一个一个的处理数据包
9. 在 pool 函数中, 驱动会一个接一个的读取网卡写到内存中的数据包, 内存中数据包的格式只有驱动知道
10. 驱动程序将内存中的数据包转换成内核网络模块能识别的 skb 格式, 然后调用 napi_gro_receive 函数
11. napi_gro_receive 会处理 GRO 相关的内容, 也就是将可以合并的数据包进行合并, 这样就只需要调用一次协议栈。然后判断是否开启了 RPS, 如果开启了, 将会调用 enqueue_to_backlog
12. 在 enqueue_to_backlog 函数中, 会将数据包放入 CPU 的 softnet_data 结构体的 input_pkt_queue 中, 然后返回, 如果 input_pkt_queue 满了的话, 该数据包将会被丢弃, queue的大小可以通过net.core.netdev_max_backlog 来配置
13. CPU会接着在自己的软中断上下文中处理自己 input_pkt_queue 里的网络数据（调用__netif_receive_skb_core）
14. 如果没开启RPS, napi_gro_receive会直接调用__netif_receive_skb_core
15. 看是不是有AF_PACKET类型的socket（也就是我们常说的原始套接字）, 如果有的话, 拷贝一份数据给它。tcpdump抓包就是抓的这里的包。
16. 调用协议栈相应的函数, 将数据包交给协议栈处理。
17. 待内存中的所有数据包被处理完成后（即poll函数执行完成）, 启用网卡的硬中断, 这样下次网卡再收到数据的时候就会通知CPU

enqueue_to_backlog函数也会被netif_rx函数调用, 而netif_rx正是lo设备发送数据包时调用的函数
```

##### IP 层

```
          |
          |
          ↓         promiscuous mode &&
      +--------+    PACKET_OTHERHOST (set by driver)   +-----------------+
      | ip_rcv |-------------------------------------->| drop this packet|
      +--------+                                       +-----------------+
          |
          |
          ↓
+---------------------+
| NF_INET_PRE_ROUTING |
+---------------------+
          |
          |
          ↓
      +---------+
      |         | enabled ip forword  +------------+        +----------------+
      | routing |-------------------->| ip_forward |------->| NF_INET_FORWARD |
      |         |                     +------------+        +----------------+
      +---------+                                                   |
          |                                                         |
          | destination IP is local                                 ↓
          ↓                                                 +---------------+
 +------------------+                                       | dst_output_sk |
 | ip_local_deliver |                                       +---------------+
 +------------------+
          |
          |
          ↓
 +------------------+
 | NF_INET_LOCAL_IN |
 +------------------+
          |
          |
          ↓
    +-----------+
    | UDP layer |
    +-----------+
```

```
由于是UDP包, 所以第一步会进入IP层, 然后一级一级的函数往下调


ip_rcv： ip_rcv函数是IP模块的入口函数, 在该函数里面, 第一件事就是将垃圾数据包（目的mac地址不是当前网卡, 但由于网卡设置了混杂模式而被接收进来）直接丢掉, 然后调用注册在NF_INET_PRE_ROUTING上的函数
NF_INET_PRE_ROUTING： netfilter放在协议栈中的钩子, 可以通过iptables来注入一些数据包处理函数, 用来修改或者丢弃数据包, 如果数据包没被丢弃, 将继续往下走
routing： 进行路由, 如果是目的IP不是本地IP, 且没有开启ip forward功能, 那么数据包将被丢弃, 如果开启了ip forward功能, 那将进入ip_forward函数
ip_forward： ip_forward会先调用netfilter注册的NF_INET_FORWARD相关函数, 如果数据包没有被丢弃, 那么将继续往后调用dst_output_sk函数
dst_output_sk： 该函数会调用IP层的相应函数将该数据包发送出去, 同下一篇要介绍的数据包发送流程的后半部分一样。
ip_local_deliver：如果上面routing的时候发现目的IP是本地IP, 那么将会调用该函数, 在该函数中, 会先调用NF_INET_LOCAL_IN相关的钩子程序, 如果通过, 数据包将会向下发送到UDP层
```

##### UDP 层

```
          |
          |
          ↓
      +---------+            +-----------------------+
      | udp_rcv |----------->| __udp4_lib_lookup_skb |
      +---------+            +-----------------------+
          |
          |
          ↓
 +--------------------+      +-----------+
 | sock_queue_rcv_skb |----->| sk_filter |
 +--------------------+      +-----------+
          |
          |
          ↓
 +------------------+
 | __skb_queue_tail |
 +------------------+
          |
          |
          ↓
  +---------------+
  | sk_data_ready |
  +---------------+
```

```
udp_rcv： udp_rcv函数是UDP模块的入口函数, 它里面会调用其它的函数, 主要是做一些必要的检查, 其中一个重要的调用是__udp4_lib_lookup_skb, 该函数会根据目的IP和端口找对应的socket, 如果没有找到相应的socket, 那么该数据包将会被丢弃, 否则继续
sock_queue_rcv_skb： 主要干了两件事, 一是检查这个socket的receive buffer是不是满了, 如果满了的话, 丢弃该数据包, 然后就是调用sk_filter看这个包是否是满足条件的包, 如果当前socket上设置了filter, 且该包不满足条件的话, 这个数据包也将被丢弃（在Linux里面, 每个socket上都可以像tcpdump里面一样定义filter, 不满足条件的数据包将会被丢弃）
__skb_queue_tail： 将数据包放入socket接收队列的末尾
sk_data_ready： 通知socket数据包已经准备好


调用完sk_data_ready之后, 一个数据包处理完成, 等待应用层程序来读取, 上面所有函数的执行过程都在软中断的上下文中。
```

##### socket

```
应用层一般有两种方式接收数据, 一种是recvfrom函数阻塞在那里等着数据来, 这种情况下当socket收到通知后, recvfrom就会被唤醒, 然后读取接收队列的数据；另一种是通过epoll或者select监听相应的socket, 当收到通知后, 再调用recvfrom函数去读取接收队列的数据。两种情况都能正常的接收到相应的数据包。
```

# 网络数据包发送过程

>  以一个 UDP 包的接收过程作为示例

##### socket

```
               +-------------+
               | Application |
               +-------------+
                     |
                     |
                     ↓
+------------------------------------------+
| socket(AF_INET, SOCK_DGRAM, IPPROTO_UDP) |
+------------------------------------------+
                     |
                     |
                     ↓
           +-------------------+
           | sendto(sock, ...) |
           +-------------------+
                     |
                     |
                     ↓
              +--------------+
              | inet_sendmsg |
              +--------------+
                     |
                     |
                     ↓
             +---------------+
             | inet_autobind |
             +---------------+
                     |
                     |
                     ↓
               +-----------+
               | UDP layer |
               +-----------+
```

```
socket(...)： 创建一个socket结构体, 并初始化相应的操作函数, 由于我们定义的是UDP的socket, 所以里面存放的都是跟UDP相关的函数

sendto(sock, ...)： 应用层的程序（Application）调用该函数开始发送数据包, 该函数数会调用后面的inet_sendmsg

inet_sendmsg： 该函数主要是检查当前socket有没有绑定源端口, 如果没有的话, 调用inet_autobind分配一个, 然后调用UDP层的函数

inet_autobind： 该函数会调用socket上绑定的get_port函数获取一个可用的端口, 由于该socket是UDP的socket, 所以get_port函数会调到UDP代码里面的相应函数。
```

##### UDP 层

```
                     |
                     |
                     ↓
              +-------------+
              | udp_sendmsg |
              +-------------+
                     |
                     |
                     ↓
          +----------------------+
          | ip_route_output_flow |
          +----------------------+
                     |
                     |
                     ↓
              +-------------+
              | ip_make_skb |
              +-------------+
                     |
                     |
                     ↓
         +------------------------+
         | udp_send_skb(skb, fl4) |
         +------------------------+
                     |
                     |
                     ↓
                +----------+
                | IP layer |
                +----------+
```

```
udp_sendmsg： udp模块发送数据包的入口, 该函数较长, 在该函数中会先调用ip_route_output_flow获取路由信息（主要包括源IP和网卡）, 然后调用ip_make_skb构造skb结构体, 最后将网卡的信息和该skb关联。

ip_route_output_flow： 该函数会根据路由表和目的IP, 找到这个数据包应该从哪个设备发送出去, 如果该socket没有绑定源IP, 该函数还会根据路由表找到一个最合适的源IP给它。 如果该socket已经绑定了源IP, 但根据路由表, 从这个源IP对应的网卡没法到达目的地址, 则该包会被丢弃, 于是数据发送失败, sendto函数将返回错误。该函数最后会将找到的设备和源IP塞进flowi4结构体并返回给udp_sendmsg

ip_make_skb： 该函数的功能是构造skb包, 构造好的skb包里面已经分配了IP包头, 并且初始化了部分信息（IP包头的源IP就在这里被设置进去）, 同时该函数会调用__ip_append_dat, 如果需要分片的话, 会在__ip_append_data函数中进行分片, 同时还会在该函数中检查socket的send buffer是否已经用光, 如果被用光的话, 返回ENOBUFS

udp_send_skb(skb, fl4) 主要是往skb里面填充UDP的包头, 同时处理checksum, 然后调用IP层的相应函数。
```

##### IP 层

```
          |
          |
          ↓
   +-------------+
   | ip_send_skb |
   +-------------+
          |
          |
          ↓
  +-------------------+       +-------------------+       +---------------+
  | __ip_local_out_sk |------>| NF_INET_LOCAL_OUT |------>| dst_output_sk |
  +-------------------+       +-------------------+       +---------------+
                                                                  |
                                                                  |
                                                                  ↓
 +------------------+        +----------------------+       +-----------+
 | ip_finish_output |<-------| NF_INET_POST_ROUTING |<------| ip_output |
 +------------------+        +----------------------+       +-----------+
          |
          |
          ↓
  +-------------------+      +------------------+       +----------------------+
  | ip_finish_output2 |----->| dst_neigh_output |------>| neigh_resolve_output |
  +-------------------+      +------------------+       +----------------------+
                                                                   |
                                                                   |
                                                                   ↓
                                                           +----------------+
                                                           | dev_queue_xmit |
                                                           +----------------+
```

```
ip_send_skb： IP模块发送数据包的入口, 该函数只是简单的调用一下后面的函数

__ip_local_out_sk： 设置IP报文头的长度和checksum, 然后调用下面netfilter的钩子

NF_INET_LOCAL_OUT： netfilter的钩子, 可以通过iptables来配置怎么处理该数据包, 如果该数据包没被丢弃, 则继续往下走

dst_output_sk： 该函数根据skb里面的信息, 调用相应的output函数, 在我们UDP IPv4这种情况下, 会调用ip_output

ip_output： 将上面udp_sendmsg得到的网卡信息写入skb, 然后调用NF_INET_POST_ROUTING的钩子

NF_INET_POST_ROUTING： 在这里, 用户有可能配置了SNAT, 从而导致该skb的路由信息发生变化

ip_finish_output： 这里会判断经过了上一步后, 路由信息是否发生变化, 如果发生变化的话, 需要重新调用dst_output_sk（重新调用这个函数时, 可能就不会再走到ip_output, 而是走到被netfilter指定的output函数里, 这里有可能是xfrm4_transport_output）, 否则往下走

ip_finish_output2： 根据目的IP到路由表里面找到下一跳(nexthop)的地址, 然后调用__ipv4_neigh_lookup_noref去arp表里面找下一跳的neigh信息, 没找到的话会调用__neigh_create构造一个空的neigh结构体

dst_neigh_output： 在该函数中, 如果上一步ip_finish_output2没得到neigh信息, 那么将会走到函数neigh_resolve_output中, 否则直接调用neigh_hh_output, 在该函数中, 会将neigh信息里面的mac地址填到skb中, 然后调用dev_queue_xmit发送数据包

neigh_resolve_output： 该函数里面会发送arp请求, 得到下一跳的mac地址, 然后将mac地址填到skb中并调用dev_queue_xmit
```

##### netdevice子系统

```
                          |
                          |
                          ↓
                   +----------------+
  +----------------| dev_queue_xmit |
  |                +----------------+
  |                       |
  |                       |
  |                       ↓
  |              +-----------------+
  |              | Traffic Control |
  |              +-----------------+
  | loopback              |
  |   or                  +--------------------------------------------------------------+
  | IP tunnels            ↓                                                              |
  |                       ↓                                                              |
  |            +---------------------+  Failed   +----------------------+         +---------------+
  +----------->| dev_hard_start_xmit |---------->| raise NET_TX_SOFTIRQ |- - - - >| net_tx_action |
               +---------------------+           +----------------------+         +---------------+
                          |
                          +----------------------------------+
                          |                                  |
                          ↓                                  ↓
                  +----------------+              +------------------------+
                  | ndo_start_xmit |              | packet taps(AF_PACKET) |
                  +----------------+              +------------------------+
```

```
dev_queue_xmit： netdevice子系统的入口函数, 在该函数中, 会先获取设备对应的qdisc, 如果没有的话（如loopback或者IP tunnels）, 就直接调用dev_hard_start_xmit, 否则数据包将经过Traffic Control模块进行处理

Traffic Control： 这里主要是进行一些过滤和优先级处理, 在这里, 如果队列满了的话, 数据包会被丢掉, 详情请参考文档, 这步完成后也会走到dev_hard_start_xmit

dev_hard_start_xmit： 该函数中, 首先是拷贝一份skb给“packet taps”, tcpdump就是从这里得到数据的, 然后调用ndo_start_xmit。如果dev_hard_start_xmit返回错误的话（大部分情况可能是NETDEV_TX_BUSY）, 调用它的函数会把skb放到一个地方, 然后抛出软中断NET_TX_SOFTIRQ, 交给软中断处理程序net_tx_action稍后重试（如果是loopback或者IP tunnels的话, 失败后不会有重试的逻辑）

ndo_start_xmit： 这是一个函数指针, 会指向具体驱动发送数据的函数

```

##### Device Driver

```
ndo_start_xmit会绑定到具体网卡驱动的相应函数, 到这步之后, 就归网卡驱动管了, 不同的网卡驱动有不同的处理方式, 这里不做详细介绍, 其大概流程如下：
    将skb放入网卡自己的发送队列
    通知网卡发送数据包
    网卡发送完成后发送中断给CPU
    收到中断后进行skb的清理工作


在网卡驱动发送数据包过程中, 会有一些地方需要和netdevice子系统打交道, 比如网卡的队列满了, 需要告诉上层不要再发了, 等队列有空闲的时候, 再通知上层接着发数据。
```

```
SO_SNDBUF: 从上面的流程中可以看出来, 对于UDP来说, 没有一个对应send buffer存在, SO_SNDBUF只是一个限制, 当这个socket分配的skb占用的内存超过这个值的时候, 会返回ENOBUFS, 所以说只要不出现ENOBUFS错误, 把这个值调大没有意义。从sendto函数的帮助文件里面看到这样一句话：(Normally, this does not occur in Linux. Packets are just silently dropped when a device queue overflows.)。这里的device queue应该指的是Traffic Control里面的queue, 说明在linux里面, 默认的SO_SNDBUF值已经够queue用了, 疑问的地方是, queue的长度和个数是可以配置的, 如果配置太大的话, 按道理应该有可能会出现ENOBUFS的情况。

txqueuelen: 很多地方都说这个是控制qdisc里queue的长度的, 但貌似只是部分类型的qdisc用了该配置, 如linux默认的pfifo_fast。

hardware RX: 一般网卡都有一个自己的ring queue, 这个queue的大小可以通过ethtool来配置, 当驱动收到发送请求时, 一般是放到这个queue里面, 然后通知网卡发送数据, 当这个queue满的时候, 会给上层调用返回NETDEV_TX_BUSY

packet taps(AF_PACKET): 当第一次发送数据包和重试发送数据包时, 都会经过这里, 如果发生重试的情况的话, 不确定tcpdump是否会抓到两次包, 按道理应该不会, 可能是我哪里没看懂
```

# 网卡

1. 网络设备像一个管道 (pipe) , 从任意一端收到的数据将从另一端发送出去
2. Linux 内核中有一个网络设备管理模块, 处于网络设备驱动和协议栈之间, 负责衔接它们之间的数据交互, 管理网卡(物理/虚拟)
3. 物理/虚拟网卡需要有驱动才能工作, 驱动是加载到内核中的模块, 负责衔接网卡和内核的网络模块, 驱动在加载的时候将自己注册进网络模块, 当相应的网卡收到数据包时, 网络模块会调用相应的驱动程序处理数据。
4. 物理网卡 eth0, 它的两端分别是内核协议栈（通过网络设备管理模块间接的通信）和外面的物理网络, 从物理网络收到的数据, 会转发给内核协议栈, 而应用程序从协议栈发过来的数据将会通过物理网络发送出去
5. 虚拟网络设备, 对于Linux内核网络设备管理模块来说, 虚拟设备和物理设备没有区别, 都是网络设备, 都能配置IP, 从网络设备来的数据, 都会转发给协议栈, 协议栈过来的数据, 也会交由网络设备发送出去, 至于是怎么发送出去的, 发到哪里去, 那是设备驱动的事情, 跟Linux内核就没关系了, 所以说虚拟网络设备的一端也是协议栈, 而另一端是什么取决于虚拟网络设备的驱动实现

#### 物理网卡设备

##### 是什么

<img src=".\image\物理网卡.jpg" alt="物理网卡" style="zoom:60%;" />

```
1. 物理网卡是硬件设备, 位于硬件层
2. 两端分别连接内核的网络协议栈和外界网络
3. 以 0 1 形式的比特流收发数据
4. 物理网卡需要通过网卡驱动在内核中注册后才能工作,可以为物理网卡配置属性 (IP, 子网掩码等), 这些都配置在内核的网络协议栈中.
```

##### 用户进程和网卡传输过程

<img src=".\image\用户进程和物理网卡传输.jpg" alt="用户进程和物理网卡传输" style="zoom:60%;" />

```
1. 用户进程发送数据时, 数据从用户空间写入到内核的网络协议栈, 再从网络协议栈传输到网卡, 最后发送出去
2. 用户进程等待外界响应数据时, 数据从网卡流入, 传输至内核的网络协议栈, 最后数据写入用户空间被用户进程读取
3. 内核和用户空间的数据传输, 由内核占用 CPU 来完成
4. 内核和网卡之间的数据传输, 由网卡的 DMA 来完成, 不需要占用过多的 CPU
```

#### 虚拟网卡设备

##### 是什么

<img src=".\image\虚拟网卡.jpg" alt="虚拟网卡" style="zoom:60%;" />

```
1. 内核直接创建虚拟网卡, 需要为虚拟网卡提供网卡驱动程序, 使其在内核中注册成为网卡设备, 才能工作
2. 可以像物理网卡一样配置属性, 比 IP, 子网掩码等.
3. 虚拟网卡两端是内核的网络协议栈和用户空间, 它只负责在内核网络协议栈和用户空间的程序之间传递数据. 用户空间的程序是无法直接对数据包做任何封装和解封操作的, 只能通过内核的网络协议栈来完成封装和解封后使用数据包
4. 也可以将数据传输到本机的另一个虚拟网卡/ 物理网卡 / 虚拟交换机 等其他虚拟设备上 
```

##### 与物理网卡区别

```
1. 物理网卡以比特流的方式和外界传输数据
2. 虚拟网卡在内存中拷贝数据(在内核之间和应用程序之间传输数据)
3. 物理网卡能够直接收发外界的数据. 虚拟网卡不具备物理网卡以比特流方式传输数据的硬件功能, 所以无法通过虚拟网卡向外界发送数据, 外界数据也无法直接发送到虚拟网卡上.
```

# tun/tap

##### tun 和 tap 是什么

```
1. tun 和 tap 都是虚拟网络网卡, 完全由软件来实现, 功能和硬件实现完全没有差别, 都可以配置属性, 由 Linux 网络设备管理模块统一管理
2. tap 
	操作第二层数据包, 读写链路层数据包, 如以太网数据帧
	拥有 MAC 层功能, 可以与物理网卡做桥转发, 支持 MAC 层广播
3. tun
	操作第三层数据包, 模拟了网络层设备, 如 IP 数据封包
	没有 MAC 地址, 只能工作在 IP 层, 无法与物理网卡做桥转发
4. tun/tap 设备常用于 VPN
```

##### 应用与 tun/tap 交互

<img src=".\image\tap和tun原理.webp" alt="tap和tun原理" style="zoom:60%;" />

```
1. 应用程序与 tun/tap 交互必须使用驱动
2. Linux (2.6.x 之后) 给 tap/tun 提供了两种驱动:
    1. 网卡驱动
    2. 字符设备驱动
    	tap：/dev/tap0
    	tun：/dev/net/tun
3. 应用和驱动的交互方式
    1. 虚拟网络接口 (通过 Socket API 调用接口发送数据给驱动, 同时这些数据包会被原样写入数据到字符设备文件)
    2. 字符设备文件 (写入数据到字符设备文件, 同时这些数据包会被原样发送到虚拟接口上)    


当应用程序打开设备文件时, 驱动程序就会创建并注册相应的虚拟设备接口, 一般以 tunX 或 tapX 命名
当应用程序关闭设备文件时, 驱动自动删除 tunX 和 tapX 设备, 还会删除已经建立起来的路由等信息
```

##### tun/tap 消息转发

```
A 和 B 是用户层的应用程序 
socket、协议栈（Network Protocol Stack）和网络设备（eth0 和 tun0）部分都在内核层, 其实socket是协议栈的一部分, 这里分开来的目的是为了看的更直观
tun0 是 Tun/Tap 虚拟设备 ip 192.168.3.11
物理网卡 eth0 ip 10.32.0.11


+----------------------------------------------------------------+
|                                                                |
|  +--------------------+      +--------------------+            |
|  | User Application A |      | User Application B |<-----+     |
|  +--------------------+      +--------------------+      |     |
|               | 1                    | 5                 |     |
|...............|......................|...................|.....|
|               ↓                      ↓                   |     |
|         +----------+           +----------+              |     |
|         | socket A |           | socket B |              |     |
|         +----------+           +----------+              |     |
|                 | 2               | 6                    |     |
|.................|.................|......................|.....|
|                 ↓                 ↓                      |     |
|             +------------------------+                 4 |     |
|             | Network Protocol Stack |                   |     |
|             +------------------------+                   |     |
|                | 7                 | 3                   |     |
|................|...................|.....................|.....|
|                ↓                   ↓                     |     |
|        +----------------+    +----------------+          |     |
|        |      eth0      |    |      tun0      |          |     |
|        +----------------+    +----------------+          |     |
|    10.32.0.11  |                   |   192.168.3.11      |     |
|                | 8                 +---------------------+     |
|                |                                               |
+----------------|-----------------------------------------------+
                 ↓
         Physical Network
```

```
数据包选择走哪个网络设备完全由路由表控制, 如果想让某些网络流量走应用程序 B 的转发流程, 需要配置路由表让这部分数据走 tun0
这个例子中, 应用 A 发到 192.168.3.0/24 网络的数据通过应用 B 这个隧道, 利用 10.32.0.11 发到远端网络的 10.33.0.1, 再由 10.33.0.1 转发给相应的设备, 从而实现VPN
```

1. A 通过 socket A 发送了一个数据包, 目的 IP 是 192.168.3.1
2. socket A 将这个数据包发给 Network Protocol Stack
3. Network Protocol Stack 根据数据包的目的 IP 匹配本地路由规则, 知道这个数据包应该由 tun0 出去, 于是将数据包交给 tun0
4. tun0 收到数据包之后, 发现另一端被 B 打开了, 于是将数据包发给了进程 B
5. B 收到数据包之后, 做一些跟业务相关的处理(数据压缩, 加密等), 然后构造一个新的数据包, 将原来的数据包嵌入在新的数据包中, 最后通过socket B将数据包转发出去, 这时候新数据包的源地址变成了eth0的地址, 而目的IP地址变成了一个其它的地址, 比如是 10.33.0.1
6. socket B将数据包丢给协议栈
7. 协议栈根据本地路由, 发现这个数据包应该要通过 eth0 发送出去, 于是将数据包交给 eth0
8. eth0通过物理网络将数据包发送出去
9. 10.33.0.1 收到数据包之后, 打开数据包读取里面的原始数据包, 并转发给本地的192.168.3.1, 然后等收到192.168.3.1的应答后, 再构造新的应答包, 并将原始应答包封装在里面, 再由原路径返回给应用程序B, 应用程序B取出里面的原始应答包, 最后返回给应用程序 A

##### vpn

<img src=".\image\vpn数据2.webp" alt="vpn数据2" style="zoom:60%;" />

<img src=".\image\vpn数据.png" alt="vpn数据" style="zoom:60%;" />

```
VPN 软件通常使用 tun 设备, 通过配置静态路由的方式将指定网段的用户数据指向 tun 设备
VPN 软件获取报文后, 进行封装/修改将数据从物理网卡发出
回来的数据 VPN 软件从协议栈中获取数据解封装得到 IP 层数据, 在通过 TUN 写回网络协议栈中, 协议栈通过 socket 返回给用户 APP
```

##### tap 应用

```
TAP 接口的典型应用场景是在虚拟化网络中。例如, 我们通过KVM创建的多个 VM（虚拟机）, 以 LinuxBridge（桥接网络）互通；实际上即是通过像 vnet0 这样的 TAP 接口来接入 LinuxBridge 的。 在这种场景下, KVM程序 就是向 TAP 接口读写数据的用户空间程序。当 VM0 向本机的 eth0 接口发送数据, KVM 会将数据发送到 TAP 接口 vnet0 , 再通过 LinuxBridge 将数据转发到 vnet1 上。然后, KVM 将数据发送到 VM1 的 eth0 口。
```

<img src=".\image\tap用于kvm虚拟机.jpg" alt="tap用于kvm虚拟机" style="zoom:80%;" />

