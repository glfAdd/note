

## zookeeper

##### 直接运行

```
# 官网
https://zookeeper.apache.org/releases.html

1. 下载
wget https://mirrors.bfsu.edu.cn/apache/zookeeper/zookeeper-3.7.0/apache-zookeeper-3.7.0-bin.tar.gz

2. 新建两个文件夹 data 和 log,用于存储zookeeper的数据和日志

3. 将conf目录下的zoo_sample.cfg文件更名为zoo.cfg, 自定义设置数据文件目录和日志文件目录
dataDir=/usr/local/software/zookeeper/data
dataLogDir=/usr/local/software/zookeeper/log

4. 在 /usr/local/software/zookeeper/bin 目录下启动服务
bin/zkServer.sh start

5. 显示 Starting zookeeper ... STARTED 但命令不一定执行成功, 需要查看状态
bin/zkServer.sh status

6. 连接 zookeeper 命令
zkCli.sh -server localhost
```

## kafka

##### 直接运行

```
# 官网
http://kafka.apache.org/downloads.html
# 源代码, 需要编译
kafka-2.8.0-src.tgz 
# 编译好, 可以直接使用
kafka_2.13-2.8.0.tgz


1. 下载
wget https://mirrors.bfsu.edu.cn/apache/kafka/2.8.0/kafka_2.13-2.8.0.tgz

2. 启动kafka需要先启动zookeeper

3. 启动kafka
nohup bin/kafka-server-start.sh config/server.properties > /dev/null 2>&1 &
```

- 通过zookeeper检测 kafka 是否启动成功

```
1. zookeeper 客户端连接 zookeeper
./zkCli.sh -server localhost:2181

2. 查看是否有 kafka 创建的数据
[zk: localhost:2181(CONNECTED) 0] ls /brokers
[ids, seqid, topics]

3. 如果显示有值, 则表示 broker.id 为 0 的broker已经加入了kafka集群中
[zk: localhost:2181(CONNECTED) 1] ls /brokers/ids 
[0]
```

- 配置文件 config/server.properties

```
broker.id=0

# 存储Kafka的数据文件的目录
log.dirs=/opt/kafka_2.13-2.8.0/data

zookeeper.connect=localhost:2181

# 外网无法访问 (x.x.x.x 是服务器的ip)
advertised.listeners=PLAINTEXT://x.x.x.x:9092
```

##  redis

## filebeat

##### 直接运行

```
1. 官网下载: https://www.elastic.co/cn/downloads/beats/filebeat
wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.12.1-linux-x86_64.tar.gz

2. 启动
./filebeat -e -c filebeat.yml
# 将所有标准输出及标准错误输出到/dev/null空设备，即没有任何输出
nohup ./filebeat -e -c filebeat.yml >/dev/null 2>&1 & 
nohup ./filebeat -e -c filebeat.yml > filebeat.log &

参数
-c：配置文件位置
-path.logs：日志位置
-path.data：数据位置
-path.home：家位置
-e：关闭日志输出
-d 选择器：启用对指定选择器的调试。 对于选择器，可以指定逗号分隔的组件列表，也可以使用-d“*”为所有组件启用调试.例如，-d“publish”显示所有“publish”相关的消息。
```

##### filebeat.conf

```conf

```

##### filebeat.yml(写入 logstash)

```
# 日志输入配置
filebeat.inputs:
- type: log
  enabled: true
  paths:
  - /home/glfadd/Desktop/logs/*/*.log

# 日志输出配置(采用 logstash 收集日志，5044为logstash端口
output.logstash:
  hosts: ['localhost:5044']
 
```

##### filebeat.yml (写入 reidis)

- 参数说明
  - paths: 日志文件路径
  - fields: 表示在filebeat收集的日志中多增加一个字段log_source,其值是messages, 用来在logstash的output输出到elasticsearch中判断日志的来源，从而建立相应的索引
  - fields_under_root: 设置为true，表示上面新增的字段是顶级参数
  - output: 日志输出

```yml
filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /home/glfadd/Desktop/logs/*.log
    fields:
      log_source: messages
      app_id: "okr"
      deploy_id: "ork_01"
      host_ip: "123.2.45.1"
    fields_under_root: true
output.redis:
  #hosts: ["10.10.167.37:6379"]
  hosts: ["localhost:6379"]
  key: "elk_%{[app_id]}"
  db: 0
```

##### filebeat.yml (写入 kafka)

```
filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /home/glfadd/Desktop/logs/*.log
    fields:
      log_source: messages
      app_id: "okr"
      deploy_id: "test_01"
      host_ip: "22.22.22.22"
    fields_under_root: true
    multiline.pattern: '(^Traceback)|(^[0-9]{4}-[0-9]{2}-[0-9]{2}.{1}[0-9]{2}:[0-9]{2}:[0-9]{2}[\.\,]{1})'
    multiline.negate: true
    multiline.match: after
    #最多合并500行
    multiline.max_lines: 100
    #5s无响应则取消合并
    multiline.timeout: 3s
    multiline.flush_pattern: '(^Traceback)|(^[0-9]{4}-[0-9]{2}-[0-9]{2}.{1}[0-9]{2}:[0-9]{2}:[0-9]{2}[\.\,]{1})'
output.kafka:
  enabled: true
  hosts: ["10.10.167.37:9092"]
  topic: "elk_%{[app_id]}"
  partition.round_robin:
    reachable_only: true
  worker: 2
  required_acks: 1
  compression: gzip
  max_message_bytes: 10000000
```

## logstash

##### 直接运行

```
1. 官网下载: https://www.elastic.co/cn/downloads/logstash
wget https://artifacts.elastic.co/downloads/logstash/logstash-7.12.1-linux-x86_64.tar.gz

2. 运行
./bin/logstash -f redis-input.conf
./bin/logstash -f kafka-input.conf
```

##### pipelines.yml

```
- pipeline.id: kafka_input
  queue.type: persisted
  path.config: "/opt/logstash-7.12.1/config/kafka-input.conf"
- pipeline.id: redis_input
  queue.type: persisted
  path.config: "/opt/logstash-7.12.1/config/redis-input.conf"
```

```
# pipeline线程数，适当增加不超过pipeline 线程数, 如几倍 cpu 内核数
pipeline.workers: 8
# 实际output时的线程数
pipeline.output.workers: 8
# 每次发送的事件数
pipeline.batch.size: 3000
# 发送延时
pipeline.batch.delay: 1
```



##### logstash.conf 所有配置参数

```
input {
  jdbc {
	### 使用EVAL从Redis返回的事件数，此属性仅在list模式下起作用。
    batch_count => 
	### 以“旧名称”⇒“新名称”的形式配置重命名的redis命令。
	command_map => 
	### 指定列表或频道。["list", "channel", "pattern_channel"]
	data_type => 
	### Redis数据库号。
	db => 
	### Redis服务器的主机名。
	host => 
	### Redis服务器的unix套接字路径。
	path => 
	### Redis列表或通道的名称。
	key => 
	### 用于验证的密码。默认情况下没有身份验证。要连接的端口。
	password => 
	### 要连接的端口。
	port => 
	### 启用SSL支持。
	ssl => 
	### 运行线程
	threads => 
	### 超时时间
	timeout => 
  }
}
```

##### 读取 redis, 在屏幕输出, 并写入到 redis

```
input {
	redis {
		key => "filebeat_test"
		host => "localhost"
		port => 6379
		db => 0
		data_type => "list"
	}
}
output {
    stdout { codec => rubydebug }
    redis {
		key => "filebeat_test"
		host => "localhost"
		port => 6379
		db => 1
		data_type => "list"
    }
}
```

##### 读取redis写入elasticsearch

```
input {
  redis {
    key => "filebeat_test"
    host => "localhost"
    port => 6379
    db => 0
    data_type => "list"
  }
}
filter {
  #grok {
  #  patterns_dir => ["./patterns"]
  #  match => { "message" => "%{CREATETIME:create_time}" }
  #}
  grok {
    match => { "message" => "(?<request_id>[0-9]{13})" }
  }
  grok {
    match => { "message" => "(?<temp_1>elast_alert_type[\'\"\:])(?<temp_2>[\'\"\:\s]+)(?<elast_alert_type>\w+)"}
  }
  mutate {
    convert => {
      "elast_alert_type" => "string"
    }
    remove_field => ["temp_1"]
    remove_field => ["temp_2"]
    remove_field => ["log"]
    remove_field => ["agent"]
    remove_field => ["@version"]
    remove_field => ["log_source"]
    remove_field => ["ecs"]
    remove_field => ["input"]
    remove_field => ["host"]
    remove_field => ["tags"]
  }
}
output {
#  stdout { codec => rubydebug }
  elasticsearch {
    action => "index"
    index => "%{app_id}-%{+YYYY-MM}"
    hosts => ["http://localhost:9200"]
  }
}
```

##### 读取 kafka 写入 elasticsearch

```
input {
  kafka {
    bootstrap_servers => "localhost:9092"
    #topics_pattern  => "elk_test"
    topics_pattern  => "elk_.*"
    consumer_threads => 5
    decorate_events => true
    codec => "json"
    auto_offset_reset => "latest"
    group_id => "elk1"
  }
}
filter {
  #grok {
  #  patterns_dir => ["./patterns"]
  #  match => { "message" => "%{CREATETIME:create_time}" }
  #}
  grok {
    match => { "message" => "(?<request_id>[0-9]{13})" }
  }
  grok {
    match => { "message" => "(?<temp_1>elast_alert_type[\'\"\:])(?<temp_2>[\'\"\:\s]+)(?<elast_alert_type>\w+)"}
  }
  mutate {
    convert => {
      "elast_alert_type" => "string"
    }
    remove_field => ["temp_1"]
    remove_field => ["temp_2"]
    remove_field => ["log"]
    remove_field => ["agent"]
    remove_field => ["@version"]
    remove_field => ["log_source"]
    remove_field => ["ecs"]
    remove_field => ["input"]
    remove_field => ["host"]
    remove_field => ["tags"]
  }
}
output {
  stdout { codec => rubydebug }
  elasticsearch {
    action => "index"
    index => "%{app_id}-%{+YYYY-MM}"
    hosts => ["http://localhost:9200"]
  }
}
```

## elasticsearch

##### 直接运行

```
1. 官网下载: https://www.elastic.co/cn/downloads/elasticsearch
wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.12.1-linux-x86_64.tar.gz

3. 配置文件 ./config/elasticsearch.yml

2. 运行
./bin/elasticsearch 
./bin/elasticsearch -d -p pid
	-d 守护程序运行
	-p 将进程 ID 记录到 pid 文件

4. 检查运行状态
curl -X GET "localhost:9200/?pretty"

```

##### 配置

```
elasticsearch.yml 			ES 的配置
jvm.options 				ES JVM 配置
log4j2.properties 			ES 日志配置



# 修改 elasticsearch.yml 文件设置跨域访问
http.cors.enabled: true
http.cors.allow-origin: "*"



# 修改 jvm.options 文件设置使用内存大小, 默认使用内存多
-Xms1g
-Xmx2g
```

##### 分词器

```


```

##### 问题1

```
问题:
supervosor启动时, 状态总是 RUNNINT 和 STARTING 互相切换. 通过 supervisor web 页面查看日志
[2021-05-12T15:04:22,685][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [10-10-167-37] uncaught exception in thread [main]
org.elasticsearch.bootstrap.StartupException: java.lang.RuntimeException: can not run elasticsearch as root
	at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:163) ~[elasticsearch-7.12.1.jar:7.12.1]
	at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:150) ~[elasticsearch-7.12.1.jar:7.12.1]
	at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:75) ~[elasticsearch-7.12.1.jar:7.12.1]
	at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:116) ~[elasticsearch-cli-7.12.1.jar:7.12.1]
	at org.elasticsearch.cli.Command.main(Command.java:79) ~[elasticsearch-cli-7.12.1.jar:7.12.1]
	at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:115) ~[elasticsearch-7.12.1.jar:7.12.1]
	at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:81) ~[elasticsearch-7.12.1.jar:7.12.1]
Caused by: java.lang.RuntimeException: can not run elasticsearch as root
	at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:101) ~[elasticsearch-7.12.1.jar:7.12.1]
	at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:168) ~[elasticsearch-7.12.1.jar:7.12.1]
	at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:397) ~[elasticsearch-7.12.1.jar:7.12.1]
	at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:159) ~[elasticsearch-7.12.1.jar:7.12.1]
	... 6 more
	

原因:
因为在 Elasticsearch 5.X 之后为了安全起见，不能用 root 用户来启动，必须用非 root 用户启动


解决办法:
.ini 配置文件种指定用哪个用户启动
user=xxx
```

##### 允许外网访问

- 编辑 /etc/security/limits.conf 结尾添加如下内容

  ```
  gong soft nofile 65536
  gong hard nofile 65536
  gong soft nproc 4096
  gong hard nproc 4096
  ```

- 编辑 /etc/sysctl.conf 结尾添加如下内容

  ```
  vm.max_map_count=655360
  ```

- 编辑 config/elasticsearch.yml

  ```
  # 启动地址，如果不配置，只能本地访问
  network.host: 0.0.0.0
  # 节点名称
  node.name: node-name
  # 节点列表
  discovery.seed_hosts: ["192.168.209.132"]
  # 初始化时master节点的选举列表
  cluster.initial_master_nodes: [ "node-name" ]
  # 集群名称
  cluster.name: cluster-name
  # 对外提供服务的端口
  http.port: 9200
  # 内部服务端口
  transport.port: 9300    
  # 跨域支持
  http.cors.enabled: true
  # 跨域访问允许的域名地址（正则）
  http.cors.allow-origin: /.*/
  ```

## elasticsearch-head

##### google chrome 浏览器插件

```
使用方法
https://blog.csdn.net/qq_27088383/article/details/106291602
```

## Kibana

##### 直接运行

```
1. 下载
wget https://artifacts.elastic.co/downloads/kibana/kibana-7.12.1-linux-x86_64.tar.gz

3. 启动
./bin/kibana

4. 浏览器打开
http://localhost:5601
```

##### 配置文件 kibana.yml

```
# 配置es集群url
elasticsearch.url: "http://localhost:9200"

# 汉化
i18n.locale: zh-CN

# 外网访问
server.host: "0.0.0.0"

# Kibana 登录密码


详细参数
https://blog.csdn.net/cb2474600377/article/details/108884414
```

## supervisor

##### zookeeper.conf

```
[program:zookeeper]
directory=/opt/apache-zookeeper-3.7.0-bin
command=/opt/apache-zookeeper-3.7.0-bin/bin/zkServer.sh start-foreground
autostart=false
autorestart=false
user=glfadd
startretries=3
log_stdout=true
log_stderr=true
redirect_stderr = true
; stdout 日志文件大小，默认 50MB
stdout_logfile_maxbytes = 20MB
; stdout 日志文件备份数
stdout_logfile_backups = 20     
;指定目录不存在时无法正常启动
stdout_logfile = /opt/logs/supervisord_zookeeper.log
environment=JAVA_HOME=/home/glfadd/.sdkman/candidates/java/8.0.265-open
```

##### kafka.conf

```
[program:kafka]
directory=/opt/kafka_2.13-2.8.0
command=/opt/kafka_2.13-2.8.0/bin/kafka-server-start.sh /opt/kafka_2.13-2.8.0/config/server.properties
autostart=false
autorestart=false
user=glfadd
startretries=3
log_stdout=true
log_stderr=true
redirect_stderr = true
stdout_logfile_maxbytes = 20MB
stdout_logfile_backups = 20
stdout_logfile = /opt/logs/supervisord_kafka.log
environment=JAVA_HOME=/home/glfadd/.sdkman/candidates/java/8.0.265-open
```

##### redis.conf

```
[program:redis]
directory=/opt/redis-6.2.2/
command=/opt/redis-6.2.2/src/redis-server /opt/redis-6.2.2/redis.conf
autostart=false
autorestart=false
user=glfadd
startretries=3
log_stdout=true
log_stderr=true
redirect_stderr = true
stdout_logfile_maxbytes = 20MB
stdout_logfile_backups = 20
stdout_logfile = /opt/logs/supervisord_redis.log
```

##### filebeat.conf

```
[program:filebeat]
directory=/opt/filebeat-7.12.1-linux-x86_64
command=/opt/filebeat-7.12.1-linux-x86_64/filebeat -c /opt/filebeat-7.12.1-linux-x86_64/filebeat.yml
autostart=false
autorestart=false
user=glfadd
startretries=3
log_stdout=true
log_stderr=true
redirect_stderr = true
stdout_logfile_maxbytes = 20MB
stdout_logfile_backups = 20
stdout_logfile = /opt/logs/supervisord_filebeat.log
```

##### logstash.conf

```
[program:logstash]
directory=/opt/logstash-7.12.1
command=/opt/logstash-7.12.1/bin/logstash
autostart=false
autorestart=false
user=glfadd
startretries=3
log_stdout=true
log_stderr=true
redirect_stderr = true
stdout_logfile_maxbytes = 20MB
stdout_logfile_backups = 20
stdout_logfile = /opt/logs/supervisord_logstash.log
```

##### elasticsearch.conf

```
[program:elasticsearch]
directory=/opt/elasticsearch-7.12.1
command=/opt/elasticsearch-7.12.1/bin/elasticsearch
autostart=false
autorestart=false
user=glfadd
startretries=3
log_stdout=true
log_stderr=true
redirect_stderr = true
stdout_logfile_maxbytes = 20MB
stdout_logfile_backups = 20
stdout_logfile = /opt/logs/supervisord_elasticsearch.log
```

##### kibana.conf

```
[program:kibana]
directory=/opt/kibana-7.12.1-linux-x86_64
command=/opt/kibana-7.12.1-linux-x86_64/bin/kibana
autostart=false
autorestart=false
user=glfadd
startretries=3
log_stdout=true
log_stderr=true
redirect_stderr = true
stdout_logfile_maxbytes = 20MB
stdout_logfile_backups = 20
stdout_logfile = /opt/logs/supervisord_kibana.log
```

##### nginx.conf

```
[program:nginx]
directory=/opt/nginx-1.21.0
command=/opt/nginx-1.21.0/sbin/nginx -g 'daemon off;'
autostart=false
autorestart=false
user=root
startretries=3
log_stdout=true
log_stderr=true
redirect_stderr = true
stdout_logfile_maxbytes = 20MB
stdout_logfile_backups = 20
stdout_logfile = /opt/logs/supervisord_nginx.log
```

##### elastalert

```
[program:elastalert]
directory=/opt/elastalert
command=/home/gong/miniconda3/envs/p3613_elast_alert/bin/python -m elastalert.elastalert
autostart=true
autorestart=true
user=gong
log_stdout=true
log_stderr=true
redirect_stderr = true
stdout_logfile_maxbytes = 20MB
stdout_logfile_backups = 20
stdout_logfile = /opt/logs/supervisord_elastalert.log
```



