# 编译文件

##### 生成 .pyc 文件

> 并不是所有的 .py 文件都生成 .pyc 文件

1. 自动生成, 当 py 文件被当做模块调用时会自动生成 .pyc 文件

   ```
   ├── a.py
   ├── a.pyc
   └── b.py
   ```

   - a.py 文件

     ```python
     print('123')
     ```

   - b.py 文件

     ```python
     import a
     ```

2. 使用参数 -m

   ```bash
   $ python a.py -m
   ```

3. 通过代码来生成 pyc文件

   ```python
   # 生成单个 .pyc 文件
   compile(file[, cfile[, dfile[, doraise]]])
   
   # 批量生成pyc文件
   compile_dir(dir[, maxlevels[, ddir[, force[, rx[, quiet]]]]])
   
   import compileall
   compileall.compile_dir(r'H:/game')
   ```

##### 生成 .pyo 文件

```bash
比 pyc 文件性能高

# 是优化过的字节码文件
python -O -m py_compile file.py
```

##### 生成 .pyd 文件

```
只有 windows 平台才有
```

# 解释器

##### py 文件运行步骤

<img src=".\image\py文件运行步骤.png" alt="py文件运行步骤" style="zoom:60%;" />

```python
1. 字节码编译
	运行 .py 文件时, 将 .py 文件编译成字节码后保存为 .pyc 文件
	下一次运行时, 如果 .py 文件没有修改过, 会直接加载 .pyc 文件, 跳过编译步骤
    (生成 .pyc 文件时, 写入了一个 Long 型的变量, 记录最近修改的时间戳. 每次载入之前先检查 .py 文件和 .pyc 文件最后修改日期, 如果不一致则会生成一个新的 pyc 文件)


2. 虚拟机 Python Virtual Machine (PVM), 将 .pyc 文件发送到 PVM 执行
```

##### 解释器种类

```
CPython
PyPy
Psyco
JPython
```

# io 多路复用

```
I/O multiplexing (I/O 多路复用 )
event driven IO (事件驱动 IO): select, poll 和 epoll
提高服务器的吞吐能力
在单线程通过记录跟踪每一个 Sock (I/O流) 的状态来同时管理多个 I/O 流


一个进程可以监视多个描述符
某个描述符就绪 (读/写就绪), 能够通知程序进行相应的读写操作

可以基于一个阻塞对象, 同时在多个描述符上等待就绪, 而不是使用多个线程
(每个文件描述符一个线程，每次new一个线程)，
这样可以大大节省系统资源


```



##### 文件描述符

```
（套接字描述符）
1. File descriptor (fd) 文件描述符, 是一个索引值, 指向内核为每一个进程所维护的该进程打开文件的记录表
2. 只适用于UNIX、Linux这样的操作系统
3. 当程序打开一个现有文件或者创建一个新文件时, 内核向进程返回一个文件描述符
```

##### I/O 模型分类

```
阻塞 I/O 模型: 用户线程在读写时被阻塞
非阻塞 I/O 模型:用户线程不断发起IO请求. 数据未到达时系统返回一状态值; 数据到达后才真正读取数据
I/O 复用模型: 一个线程，通过记录I/O流的状态来同时管理多个I/O，可以提高服务器的吞吐能力。
信号驱动 I/O 模型
异步 I/O 模型
```

##### 应用场景

```
I/O 多路复用是指内核一旦发现进程指定的一个或者多个IO条件准备读取，它就通知该进程
    （1）当客户处理多个描述字时（一般是交互式输入和网络套接口），必须使用I/O复用
    （2）当一个客户同时处理多个套接口时，而这种情况是可能的，但很少出现
    （3）如果一个TCP服务器既要处理监听套接口，又要处理已连接套接口，一般也要用到I/O复用
    （4）如果一个服务器即要处理TCP，又要处理UDP，一般要使用I/O复用
    （5）如果一个服务器要处理多个服务或多个协议，一般要使用I/O复用
```

##### 传统的多进程并发处理

```
如果 1 个 I/O 流进来, 就开启 1 个进程处理这个 I/O 流. 如果有 100w 个 I/O 流进来, 要开启 100w 个进程一一对应处理这些I/O流, 占用大量资源.
```

##### IO多路复用

```
1. 不新建进程和线程情况下实现并发
2. 与多进程和多线程相比, I/O 多路复用系统开销小, 系统不必创建进程/线程, 也不必维护这些进程/线程, 从而大大减小了系统的开销
```

```

I/O 多路复用就通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。
但 select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，
而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。  
```

### select

```
同个进程能同时等待多个文件描述符, 而这些文件描述符其中的任意一个进入读就绪状态, select() 函数就可以返回
1. 时间复杂度 O(n)
2. select 几乎在所有的平台上支持
3. select 仅知道有 I/O 事件发生, 却不知道哪几个发生, select 只能轮询读取/写入数据, 同时处理的流越多轮询时间就越长, 效率较低
4. select 是通过设置或者检查存放 fd 标志位的数据结构
4. select 将用户传入的数组拷贝到内核空间, 然后查询每个 fd 对应的设备状态
5. 需要维护大量 fd, 这样会使得用户空间和内核空间在传递该结构时复制开销大
6. 单个进程可监视的 fd 数量有限制, 与操作系统有关. 32 位默认 1024, 64 位默认 2048
	查看系统限制 cat /proc/sys/fs/file-max
```

### poll

```
1. 时间复杂度O(n)
2. 与 select 本质相同, 只是 poll 基于链表来存储, 没有最大连接数的限制
如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历完所有fd后没有发现就绪设备，则挂起当前进程，直到设备就绪或者主动超时，被唤醒后它又要再次遍历fd
1. 大量的 fd 的数组被整体复制于用户态和内核地址空间之间               
2. poll 还有一个特点是“水平触发”, 如果报告了 fd 后没有被处理, 那么下次 poll 时会再次报告该 fd
```

### epoll

```
1. 时间复杂度O(1)
epoll 采用事件触发的机制, 不使用轮询. epoll 通过 epoll_ctl 注册 fd, 一旦该 fd 就绪，内核就会采用类似 callback 的回调机制来激活该 fd, epoll_wait 便可以收到通知
每个事件关联上 fd
给套接字注册回调函数, 当他们活跃时自动完成相关操作
创建 epoll 对象并注册事件监听具体事件, 以达到事件发生时触发任务的执行
```

##### epoll 两种触发模式

- LT

  ```
  1. epollLT 默认模式
  2. 只要这个 fd 还有数据可读, 每次 epoll_wait 都会返回它的事件, 提醒用户程序去操作
  ```

- ET

  ```
  1. epollET 边缘触发模式
  2. 只会提示一次, 直到下次再有数据流入之前都不会再提示了, 无论fd中是否还有数据可读. 即 read 一个 fd 时把它的 buffer 读光, 直到 read 的返回值小于请求值，或遇到 EAGAIN 错误
  ```

##### tornado 异步原理

```
tornado 底层通过epoll监听其监听列表中的所有soket，epoll是linux操作系统提供的监听多个socket的接口，因为epool ioloop可以同时监听上千个socket，加上ioloop的异步机制使得tornado成为高并发的webserver
```

### kqueue

# 多进程

```
Python实现多进程的方式主要有两种， 一种是使用 os库中的fork方法， 另一种 方法是使用multiprocessing库。这两种方法的区别在千前者仅适用于Unix/Linux操作系统，对Windows 不支持， 后者则是跨平台的实现方式。 由于本人使用的是windows电脑，因此演示结果只有multiprocessing的，但是也给出了fork的代码。本文是基于《Python爬虫与项目实践》一书。

```



##### 概念

```
PID: 进程号
TID: 线程号
0 PID：负责切换进程，开机创建的第一个进程
1 PID：直接或间接创建其他进程, 可以处理孤儿进程

僵尸进程：父进程一般等子进程结束后才结束, 用来处理结束的子进程, 父进已经结束但还没处理的子进程为僵尸进程
孤儿进程：父进程已经结束还没结束的子进程
父进程和子进程哪个先执行不一定
```

### fork

```
只在 Unix / Linux / Mac 上运行, windows 不可以
```

##### fork炸弹

```python
# 不断生成进程，最后程序会崩

import os 

while True:
    os.fork()

print("--1--")
```

### multiprocessing

```
multiprocessing 跨平台多进程模块

is_alive()：			判断进程是否还在执行
join([timeout])：	是否等待进程实例执行结束，或等待多少秒；
start()：			启动进程实例（创建子进程）；
run()：				如果没有给定target参数，对这个对象调用start()方法时，就将执行对象中的run()方法；
terminate()：		不管任务是否完成，立即终止
```

```python
from multiprocessing import Process
import os
from time import sleep


def test(name, age, **kwargs):
    for i in range(10):
        print("子进程运行中, name= %s, age=%d, pid=%d..." % (name, age, os.getpid()))
        print(kwargs)
        sleep(5)


if __name__ == "__main__":
    print("父进程 %d" % os.getpid())
    p = Process(target=test, args=("tests", 18), kwargs={"m": 20})
    p.start()
    sleep(2)
    p.terminate()
    p.join()
    print("子进程已结束")
```



```python
#coding=utf-8
from multiprocessing import Process
import time
import os

#两个子进程将会调用的两个方法
def  worker_1(interval):
    print("worker_1,父进程(%s),当前进程(%s)"%(os.getppid(),os.getpid()))
    t_start = time.time()
    time.sleep(interval) #程序将会被挂起interval秒
    t_end = time.time()
    print("worker_1,执行时间为'%0.2f'秒"%(t_end - t_start))

def  worker_2(interval):
    print("worker_2,父进程(%s),当前进程(%s)"%(os.getppid(),os.getpid()))
    t_start = time.time()
    time.sleep(interval)
    t_end = time.time()
    print("worker_2,执行时间为'%0.2f'秒"%(t_end - t_start))

#输出当前程序的ID
print("进程ID：%s"%os.getpid())

#创建两个进程对象，target指向这个进程对象要执行的对象名称，
#args后面的元组中，是要传递给worker_1方法的参数，
#因为worker_1方法就一个interval参数，这里传递一个整数2给它，
#如果不指定name参数，默认的进程对象名称为Process-N，N为一个递增的整数
p1=Process(target=worker_1,args=(2,))
p2=Process(target=worker_2,name="dongGe",args=(1,))

#使用"进程对象名称.start()"来创建并执行一个子进程，
#这两个进程对象在start后，就会分别去执行worker_1和worker_2方法中的内容
p1.start()
p2.start()

#同时父进程仍然往下执行，如果p2进程还在执行，将会返回True
print("p2.is_alive=%s"%p2.is_alive())

#输出p1和p2进程的别名和pid
print("p1.name=%s"%p1.name)
print("p1.pid=%s"%p1.pid)
print("p2.name=%s"%p2.name)
print("p2.pid=%s"%p2.pid)

#join括号中不携带参数，表示父进程在这个位置要等待p1进程执行完成后，
#再继续执行下面的语句，一般用于进程间的数据同步，如果不写这一句，
#下面的is_alive判断将会是True，在shell（cmd）里面调用这个程序时
#可以完整的看到这个过程，大家可以尝试着将下面的这条语句改成p1.join(1)，
#因为p2需要2秒以上才可能执行完成，父进程等待1秒很可能不能让p1完全执行完成，
#所以下面的print会输出True，即p1仍然在执行
p1.join()
print("p1.is_alive=%s"%p1.is_alive())
运行结果
进程ID：19866
p2.is_alive=True
p1.name=Process-1
p1.pid=19867
p2.name=dongGe
p2.pid=19868
worker_1,父进程(19866),当前进程(19867)
worker_2,父进程(19866),当前进程(19868)
worker_2,执行时间为'1.00'秒
worker_1,执行时间为'2.00'秒
p1.is_alive=False
--------------------------------------------
创建-Process子类
创建新的进程还能够使用类的方式，可以自定义一个类，继承Process类，每次实例化这个类的时候，就等同于实例化一个进程对象，请看下面的实例：

from multiprocessing import Process
import time
import os

#继承Process类
class Process_Class(Process):
    #重写__init__方法。调用父类__init__方法
    def __init__(self,interval):
        Process.__init__(self)
        self.interval = interval

    #重写Process类的run()方法
    def run(self):
        print("子进程(%s) 开始执行，父进程为（%s）"%(os.getpid(),os.getppid()))
        t_start = time.time()
        time.sleep(self.interval)
        t_stop = time.time()
        print("(%s)执行结束，耗时%0.2f秒"%(os.getpid(),t_stop-t_start))

if __name__=="__main__":
    t_start = time.time()
    print("当前程序进程(%s)"%os.getpid())        
    p1 = Process_Class(2)
    #对一个不包含target属性的Process类执行start()方法，就会运行这个类中的run()方法，所以这里会执行p1.run()
    p1.start()
    p1.join()
    t_stop = time.time()
    print("(%s)执行结束，耗时%0.2f"%(os.getpid(),t_stop-t_start))
```

### 进程池

##### 

```
当需要创建的子进程数量不多时，可以直接利用multiprocessing中的Process动态成生多个进程。
创建很多进程时用到multiprocessing模块提供的Pool方法。
初始化Pool时，可以指定一个最大进程数，当有新的请求提交到Pool中时，如果池还没有满，那么就会创建一个新的进程用来执行该请求；但如果池中的进程数已经达到指定的最大值，那么该请求就会等待，直到池中有进程结束，才会创建新的进程来执行。
```

```python
import os
from multiprocessing import Manager, Pool


def reader(queue):
    print("reader: 子进程 %s, 父进程为:%s" % (os.getpid(), os.getppid()))
    for i in range(queue.qsize()):
        print("reader 从 Queue 获取到消息：%s" % q.get(True))


def writer(queue):
    print("writer: 子进程 %s, 父进程为:%s" % (os.getpid(), os.getppid()))
    for i in "hello":
        print("writer 往 Queue 获取到消息：%s" % i)
        queue.put(i)


if __name__ == "__main__":
    print("主进程: %s" % os.getpid())
    q = Manager().Queue()
    po = Pool()
    po.apply(writer, (q,))
    po.apply(reader, (q,))
    po.close()
    po.join()
    print('end ------------')
```



```python
apply_async(func[, args[, kwds]]) ：使用非阻塞方式调用func（并行执行，堵塞方式必须等待上一个进程退出才能执行下一个进程），args为传递给func的参数列表，kwds为传递给func的关键字参数列表；
apply(func[, args[, kwds]])：使用阻塞方式调用func
close()：关闭Pool，使其不再接受新的任务；
terminate()：不管任务是否完成，立即终止；
join()：主进程阻塞，等待子进程的退出， 必须在close或terminate之后使用；
--------------------------------------------
from multiprocessing import Pool
import os,time,random

def worker(msg):
    t_start = time.time()
    print("%s开始执行,进程号为%d"%(msg,os.getpid()))
    #random.random()随机生成0~1之间的浮点数
    time.sleep(random.random()*2) 
    t_stop = time.time()
    print(msg,"执行完毕，耗时%0.2f"%(t_stop-t_start))

po=Pool(3) #定义一个进程池，最大进程数3
for i in range(0,10):
    #Pool.apply_async(要调用的目标,(传递给目标的参数元祖,))
    #每次循环将会用空闲出来的子进程去调用目标
    po.apply_async(worker,(i,))

print("----start----")
po.close() #关闭进程池，关闭后po不再接收新的请求
po.join() #等待po中所有子进程执行完成，必须放在close语句之后
print("-----end-----")
--------------------------------------------
from multiprocessing import Pool
import os,time,random

def worker(msg):
    t_start = time.time()
    print("%s开始执行,进程号为%d"%(msg,os.getpid()))
    #random.random()随机生成0~1之间的浮点数
    time.sleep(random.random()*2) 
    t_stop = time.time()
    print(msg,"执行完毕，耗时%0.2f"%(t_stop-t_start))

po=Pool(3) #定义一个进程池，最大进程数3
for i in range(0,10):
    po.apply(worker,(i,))

print("----start----")
po.close() #关闭进程池，关闭后po不再接收新的请求
po.join() #等待po中所有子进程执行完成，必须放在close语句之后
print("-----end-----")
```

### 进程间通信

##### 

```

```



```
可以使用 multiprocessing 模块的 Queue 实现多进程之间的数据传递, Queue 本身是一个消息列队程序

```

##### 函数

```python
Queue.qsize()：							返回当前队列包含的消息数量
Queue.empty()：							队列是否为空
Queue.full()：							队列是否满了
Queue.get([block[, timeout]])：			获取队列中的一条消息，然后将其从列队中移除，block默认值为True
# 如果block为True，且没有设置timeout（单位秒），消息列队如果为空，此时程序将被阻塞（停在读取状态），直到从消息列队读到消息为止，如果设置了timeout，则会等待timeout秒，若还没读取到任何消息，则抛出"Queue.Empty"异常；
# 如果block值为False，消息列队如果为空，则会立刻抛出"Queue.Empty"异常；
Queue.get_nowait()：						相当 Queue.get(False)
Queue.put(item,[block[, timeout]])：		将 item 消息写入队列, block 默认值为 True
# 如果block为True，且没有设置timeout（单位秒），消息列队如果已经没有空间可写入，此时程序将被阻塞（停在写入状态），直到从消息列队腾出空间为止，如果设置了timeout，则会等待timeout秒，若还没空间，则抛出"Queue.Full"异常；
# 如果block值为False，消息列队如果没有空间可写入，则会立刻抛出"Queue.Full"异常；
Queue.put_nowait(item)：					相当 Queue.put(item, False)
```



```python
#coding=utf-8
from multiprocessing import Queue
q=Queue(3) #初始化一个Queue对象，最多可接收三条put消息
q.put("消息1") 
q.put("消息2")
print(q.full())  #False
q.put("消息3")
print(q.full()) #True

#因为消息列队已满下面的try都会抛出异常，第一个try会等待2秒后再抛出异常，第二个Try会立刻抛出异常
try:
    q.put("消息4",True,2)
except:
    print("消息列队已满，现有消息数量:%s"%q.qsize())

try:
    q.put_nowait("消息4")
except:
    print("消息列队已满，现有消息数量:%s"%q.qsize())

#推荐的方式，先判断消息列队是否已满，再写入
if not q.full():
    q.put_nowait("消息4")

#读取消息时，先判断消息列队是否为空，再读取
if not q.empty():
    for i in range(q.qsize()):
        print(q.get_nowait())
运行结果
False
True
消息列队已满，现有消息数量:3
消息列队已满，现有消息数量:3
消息1
消息2
消息3
--------------------------------------------
一个往Queue里写数据，一个从Queue里读数据

#coding=utf-8
from multiprocessing import Process, Queue
import os, time, random

# 写数据进程执行的代码:
def write(q):
    for value in ['A', 'B', 'C']:
        print 'Put %s to queue...' % value
        q.put(value)
        time.sleep(random.random())

# 读数据进程执行的代码:
def read(q):
    while True:
        if not q.empty():
            value = q.get(True)
            print 'Get %s from queue.' % value
            time.sleep(random.random())
        else:
            break

if __name__=='__main__':
    # 父进程创建Queue，并传给各个子进程：
    q = Queue()
    pw = Process(target=write, args=(q,))
    pr = Process(target=read, args=(q,))
    # 启动子进程pw，写入:
    pw.start()    
    # 等待pw结束:
    pw.join()
    # 启动子进程pr，读取:
    pr.start()
    pr.join()
    # pr进程里是死循环，无法等待其结束，只能强行终止:
    print '所有数据都写入并且读完'
运行结果
Put A to queue...
Put B to queue...
Put C to queue...
Get A from queue.
Get B from queue.
Get C from queue.
所有数据都写入并且读完
--------------------------------------------
进程池中的Queue
如果要使用Pool创建进程，就需要使用multiprocessing.Manager()中的Queue()，而不是multiprocessing.Queue()，否则会得到一条如下的错误信息：
RuntimeError: Queue objects should only be shared between processes through inheritance.
    
#coding=utf-8
#修改import中的Queue为Manager
from multiprocessing import Manager,Pool
import os,time,random

def reader(q):
    print("reader启动(%s),父进程为(%s)"%(os.getpid(),os.getppid()))
    for i in range(q.qsize()):
        print("reader从Queue获取到消息：%s"%q.get(True))

def writer(q):
    print("writer启动(%s),父进程为(%s)"%(os.getpid(),os.getppid()))
    for i in "dongGe":
        q.put(i)

if __name__=="__main__":
    print("(%s) start"%os.getpid())
    q=Manager().Queue() #使用Manager中的Queue来初始化
    po=Pool()
    #使用阻塞模式创建进程，这样就不需要在reader中使用死循环了，可以让writer完全执行完成后，再用reader去读取
    po.apply(writer,(q,))
    po.apply(reader,(q,))
    po.close()
    po.join()
    print("(%s) End"%os.getpid())
运行结果
(5933) start
writer启动(5938),父进程为(5933)
reader启动(5939),父进程为(5933)
reader从Queue获取到消息：d
reader从Queue获取到消息：o
reader从Queue获取到消息：n
reader从Queue获取到消息：g
reader从Queue获取到消息：G
reader从Queue获取到消息：e
(5933) End
```

# 多线程

##### 特性

> ```
> 一个进程下的子线程与主线程共享同一片数据空间
> 一个线程至少有一个主进程, 主线程结束意味着进程结束了
> 当线程的run()方法结束时该线程完成
> 多线程程序的执行顺序是不确定的，但可以通过别的方式来影响线程调度的方式。
> ```

##### 什么是线程

一个进程下的子线程与主线程共享同一片数据空间

一个线程至少有一个主进程, 主线程结束意味着进程结束了

进程是系统正在运行的任务 

##### 避免使用thread模块

```
推荐使用更高级别的threading模块，而不是用thread模块有很多原因。threading模块更加先进，有更好的线程支持，并且thread模块中的一些属性会和threading模块有冲突，另一个原因是低级别的thread模块拥有的同步原语很少（其实只有一个），而threading模块则有很多。
避免使用thread模块的另一个原因是它对于进程何时退出没有控制。当主线程结束时，所有其他线程也都强制结束，不会发出警告或者进行适当的清理。如上所述，至少threading模块能够确保重要的子线程在进程退出前结束（join）
只建议哪些想访问线程的更底层级别的专家使用thread模块。为了增强这点，在python3中改模块被重命名为_thread。
```

##### 守护线程

- 守护进程:主进程在其代码结束后就已经算运行完毕了（守护进程在此时就被回收）,然后主进程会一直等非守护的子进程都运行完毕后回收子进程的资源(否则会产生僵尸进程)，才会结束，
- 守护线程:主线程在其他非守护线程运行完毕后才算运行完毕（守护线程在此时就被回收）。因为主线程的结束意味着进程的结束，进程整体的资源都将被回收，而进程必须保证非守护线程都运行完毕后才能结束。

```python
设置一个线程为守护线程，就表示你在说这个线程是不重要的，在进程退出的时候，不用等待这个线程退出。 

import threading
import time

def fuc():
    print 'start'
    time.sleep(2)
    print 'end'

if __name__ == '__main__':
    t = threading.Thread(target=fuc, args=())
    t.setDaemon(True)  # 程序不等子线程结束就完成
    t.setDaemon(False)  # 程序等子线程结束才完成(默认)
    t.start()
    time.sleep(1)
    print 'main thread end'
------------------------------------------------    
start
main thread end

start
main thread end
end
```

##### join()

- join是为了保证主线程在子线程结束后在结束，保证线程的安全
- 默认参数创建线程后，不管主线程是否执行完毕，都会等待子线程执行完毕才一起退出，有无join结果一样
- 设置守护进程,则主线程执行完毕后自动退出，不会等待子线程的执行结果。而且随着主线程的退出，子线程也结束
- 如果线程daemon属性为False，则join里的timeout参数无效，主线程会一直等待子线程结束。
- 如果线程daemon属性为True，则join里的timeout参数是有效的，主线程会等待timeout时间后，结束子线程。
- 如果同时有N个子线程join（timeout），那么实际上主线程会等待的超时时间最长为N*timeout,因为每个子线程的超时开始时刻是上一个子线程的超时结束的时刻。

```python
import threading
import time

def target():
    print 'the curent threading  %s is running' % threading.current_thread().name
    time.sleep(1)
    print 'the curent threading  %s is ended' % threading.current_thread().name

print 'the curent threading  %s is running' % threading.current_thread().name
t = threading.Thread(target=target)
# t.setDaemon(True)
t.start()
t.join()
print 'the curent threading  %s is ended' % threading.current_thread().name

# join是阻塞当前线程，即使得在当前线程结束时，不会退出
the curent threading  MainThread is running
the curent threading  Thread-1 is running
the curent threading  Thread-1 is ended
the curent threading  MainThread is ended

# 如果不加join语句，那么主线程不会等到当前线程结束才结束，但却不会立即杀死该线程
the curent threading  MainThread is running
the curent threading  Thread-1 is running
the curent threading  MainThread is ended
the curent threading  Thread-1 is ended

# setDaemon(True)不加join(), 当主线程结束之后，会杀死子线程
the curent threading  MainThread is running
the curent threading  Thread-1 is running
the curent threading  MainThread is ended

# setDaemon(True)加join(), 当前线程结束时，不会退出
the curent threading  MainThread is running
the curent threading  Thread-1 is running
the curent threading  Thread-1 is ended
the curent threading  MainThread is ended

# 主线程等待1秒，就自动结束，并杀死子线程。如果join不加等待时间，t.join(),就会一直等待，一直到子线程结束，
the curent threading  MainThread is running
the curent threading  Thread-1 is running
the curent threading  MainThread is ended
```

##### ThreadLocal

```
线程自己的全局变量, 别的线程不能用, 多线程中函数传值麻烦

local = threading.local()
def func(name):
    print 'current thread:%s' % threading.currentThread().name
    local.name = name
    print "%s in %s" % (local.name,threading.currentThread().name)
t1 = threading.Thread(target=func,args=('haibo',))
t2 = threading.Thread(target=func,args=('lina',))
t1.start()
t2.start()
t1.join()
t2.join()

current thread:Thread-1
haibo in Thread-1
current thread:Thread-2
lina in Thread-2
```

##### 线程池

```
多线程处理任务不是线程越多越好，由于在切换线程的时候，需要切换上下文环境，依然会造成cpu的大量开销。为解决这个问题，线程池的概念被提出来了。预先创建好一个较为优化的数量的线程，让过来的任务立刻能够使用，就形成了线程池。

线程池是预先创建线程的一种技术。线程池在还没有任务到来之前，创建一定数量的线程，放入空闲队列中。这些线程都是处于睡眠状态，即均为启动，不消耗 CPU，而只是占用较小的内存空间。当请求到来之后，缓冲池给这次请求分配一个空闲线程，把请求传入此线程中运行，进行处理。当预先创建的线程都处于运行 状态，即预制线程不够，线程池可以自由创建一定数量的新线程，用于处理更多的请求。当系统比较闲的时候，也可以通过移除一部分一直处于停用状态的线程
```

##### 生产者消费者模型

```
1、解耦 
假设生产者和消费者分别是两个类。如果让生产者直接调用消费者的某个方法，那么生产者对于消费者就会产生依赖（也就是耦合）。将来如果消费者的代码发生变化， 可能会影响到生产者。而如果两者都依赖于某个缓冲区，两者之间不直接依赖，耦合也就相应降低了。 
2、支持并发 
由于生产者与消费者是两个独立的并发体，他们之间是用缓冲区作为桥梁连接，生产者只需要往缓冲区里丢数据，就可以继续生产下一个数据，而消费者只需要从缓冲区了拿数据即可，这样就不会因为彼此的处理速度而发生阻塞。 
3、支持忙闲不均 
缓冲区还有另一个好处。如果制造数据的速度时快时慢，缓冲区的好处就体现出来了。当数据制造快的时候，消费者来不及处理，未处理的数据可以暂时存在缓冲区中。 等生产者的制造速度慢下来，消费者再慢慢处理掉。
```

##### 信号量

```python
每次有一个线程获得信号量时，计数器-1。若计数器为0，其他线程就停止访问信号量，直到另一个线程释放信号量

# -*- coding: utf-8 -*-
import threading, time

def run(n):
    semaphore.acquire()
    time.sleep(1)
    print('run the thread:%s\n' % n)
    semaphore.release()

if __name__ == '__main__':
    semaphore = threading.BoundedSemaphore(5)  # 最多允许5个线程同时进行

    for i in range(20):
        t = threading.Thread(target=run, args=(i,))
        t.start()

while threading.active_count() != 1:
    print(threading.active_count())
    pass
else:
    print('over')
```

##### 互斥锁

```python
# -*- coding: utf-8 -*-
import threading
import os
import time

class demo():
    def __init__(self, d):
        self.dic = d

    def buy_ticket(self, ):
        time.sleep(2)
        # locks.acquire()
        print('剩余【%s】票' % self.dic['ticket'])
        if self.dic['ticket'] > 0:
            self.dic['ticket'] -= 1
            print('%s购票成功' % os.getpid())
        else:
            print('%s购票失败' % os.getpid())
        # locks.release()

if __name__ == '__main__':
    # locks = threading.Lock()
    d = demo({'ticket': 10})
    for i in range(10):
        t = threading.Thread(target=d.buy_ticket, )
        t.start()
```

##### 死锁

- 由于一个资源被多次调用，而多次调用方都未能释放该资源就会造成死锁。



### GIL

##### 是什么

- global interpreter lock 全局解释器锁, 是解释器的特性, GIL只有CPython有
- CPython 的C语言实现中, 有一部分并不是线程安全的, 因此不能完全支持并发执行, 任意时刻只允许在解释器运行一个线程
- 每个线程在执行的过程都需要先获取GIL, 保证同一时刻只有一个线程可以执行代码, 无论有多少个线程, 多线程并不是真正的并发
- 每个进程有各自独立的GIL互不干扰
- 如果Thread1是因为I/O阻塞 让出的Gil Thread2必定拿到Gil,如果 Thread1是因为ticks计数满100让出Gil 这个时候 Thread1 和 Thread2 公平竞争

##### Python 程序执行过程

```
0. 线程进行锁竞争
1. 获取GIL
2. 切换进一个线程中去运行
3. 指定数量的字节码指令, 线程主动让出控制权(可以调用time.sleep(0)来完成)
4. 把线程设置会睡眠状态（切换出线程）
5. 释放GIL
6. 重复上述步骤 
```

##### 锁释放

```
- 一个线程执行一段时间之后就要释放 GIL 让其他线程有执行的机会, 而且从获取与释放GIL的实现来看,只有持有GIL的线程主动释放GIL,其他线程才有机会获取GIL执行自己的任务
- 每次释放GIL锁, 线程进行锁竞争、切换线程, 会消耗资源。并且由于GIL锁存在
- python2.x
1.GIL的释放逻辑是当前线程遇见IO操作
2.ticks计数达到100（ticks可以看作是python自身的一个计数器, 专门做用于GIL, 每次释放后归零, 这个计数可以通过 sys.setcheckinterval 来调整）, 进行释放。
- python3.x
使用计时器（执行时间达到阈值后, 当前线程释放GIL）



4. 对于I/O密集型的线程, 每当阻塞I/O操作是解释器会释放GIL
5. 对于CPU密集型线程, 解释器会执行一定数量的字节码之后释放GIL, 其他线程获取执行的机会



GIL对多线程的影响 ?
  - 1. CPU密集型: ticks计数很快就会达到阈值, 然后触发GIL的释放与再竞争（多个线程来回切换当然是需要消耗资源的）, 单线程会比多线程快
  - 2. IO密集型: 多线程能够有效提升效率(单线程下有IO操作会进行IO等待, 造成不必要的时间浪费, 而开启多线程能在线程A等待时, 自动切换到线程B, 可以不浪费CPU的资源, 从而能提升程序执行效率)。多线程会比单线程快
```

##### 互斥锁和Gil锁的关系

- GIL锁: 保证同一时刻只有一个线程能使用到cpu
- 互斥锁: 多线程时, 保证在同一时间数据只被一个线程所持有, 保证线程的数据安全

```
假设1个进程有2个线程 Thread1,Thread2, 要修改共享的数据data, 并且有互斥锁
- 1. 假设Thread1获得GIL可以使用cpu, 这时Thread1获得互斥锁LOCK, Thread1可以改data数据(但并没有开始修改数据)
- 2. Thread1修改data数据前发生了I/O操作或者ticks计数满100, 这个时候 Thread1 让出了GIL
- 3. Thread1 和 Thread2 开始竞争 GIL
- 4. 设Thread2正好获得了GIL, 运行代码去修改共享数据date,由于Thread1有互斥锁lock, 所以Thread2无法更改共享数据date,这时Thread2让出Gil锁 , GIL锁再次发生竞争 
- 5. 假设Thread1又抢到GIL, 由于其有互斥锁Lock所以其可以继续修改共享数据data,当Thread1修改完数据释放互斥锁lock,Thread2在获得GIL与lock后才可对data进行修改
```



多线程

```python
主线程会等待所有的子线程结束后才结束
每个线程一定会有一个名字，尽管上面的例子中没有指定线程对象的name，但是python会自动为线程指定一个名字。
线程的几种状态：新建 就绪 运行 阻塞 死亡

进程线程对比
一个程序至少有一个进程,一个进程至少有一个线程.
线程的划分尺度小于进程(资源比进程少)，使得多线程程序的并发性高。
进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率
线线程不能够独立执行，必须依存在进程中
```

callback

```python
callback：回调，进程结束以后调用。主进程调用。
打断主进程正在做的事，去执行callback的事。
执行callback用的参数是触发回调用（结束的那个进程return的值）
主进程执行完callback继续做以前的事情

from multiprocessing import Pool
import time
import os

def test():
    print("---进程池中的进程---pid=%d,ppid=%d--"%(os.getpid(),os.getppid()))
    for i in range(3):
        print("----%d---"%i)
        time.sleep(1)
    return "hahah"

def test2(args):
    print("---callback func--pid=%d"%os.getpid())
    print("---callback func--args=%s"%args)

pool = Pool(3)
pool.apply_async(func=test,callback=test2)

time.sleep(5)

print("----主进程-pid=%d----"%os.getpid())
运行结果
---进程池中的进程---pid=9401,ppid=9400--
----0---
----1---
----2---
---callback func--pid=9400
---callback func--args=hahah
----主进程-pid=9400----
```



##### 避免 GIL 影响

```
############################ 是否有问题 #########################
多核多线程和单核多线程
多核多线程比单核多线程更差, 
原因是单核下多线程, 每次释放GIL, 唤醒的那个线程都能获取到GIL锁, 所以能够无缝执行, 
但多核下, CPU-1释放GIL后, 其他CPU上的线程都会进行竞争, 但GIL可能会马上又被CPU0拿到, 导致其他几个CPU上被唤醒后的线程会醒着等待到切换时间后又进入待调度状态, 这样会造成线程颠簸(thrashing), 导致效率更低

```





# 协程

- 子程序: 线程中的函数
- 线程是系统级别的它们由操作系统调度，而协程则是程序级别的由程序根据需要自己调度。
- 在同一线程内一段代码在执行过程中会中断然后跳转执行别的代码，接着在之前中断的地方继续开始执行, 这个过程称为携程，类似yield操作。
- 协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈。因此：协程能保留上一次调用时的状态（即所有局部状态的一个特定组合），每次过程重入时，就相当于进入上一次调用的状态，换种说法：进入上一次离开时所处逻辑流的位置。
- 协程的优点：
  - 无需线程上下文切换的开销，协程避免了无意义的调度，由此可以提高性能，协程的调度完全由用户控制，发生在用户空间而非内核空间，因此切换的代价非常的小。
  - 无需原子操作锁定及同步的开销
  - 方便切换控制流，简化编程模型
  - 高并发+高扩展性+低成本：一个CPU支持上万的协程都不是问题。所以很适合用于高并发处理。
  - 一个线程可以有多个协程，当任务被堵塞的时候执行下一个任务，当恢复的时候再回来执行这个任务，任务之间的切换只需要保存每个任务的上下文内容, 这样就完全没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快
- 协程的缺点:
  - 无法利用多核资源：协程的本质是个单线程,它不能同时将 单个CPU 的多个核用上,协程需要和进程配合才能运行在多CPU上。



```python
协程，又称微线程，纤程。是比线程更小的执行单元，自带CPU上下文。

在一个线程中的某个函数，可以在任何地方保存当前函数的一些临时变量等信息，然后切换到另外一个函数中执行，注意不是通过调用函数的方式做到的，并且切换的次数以及什么时候再切换到原来的函数都由开发者自己确定

协程和线程差异
线程切换从系统层面远不止保存和恢复CPU上下文这么简单。操作系统为了程序运行的高效性每个线程都有自己缓存Cache等等数据，操作系统还会帮你做这些数据的恢复操作。 所以线程的切换非常耗性能。
协程的切换只是单纯的操作CPU的上下文，所以一秒钟切换个上百万次系统都抗的住。

协程是完成并发的一种方式，比进程和线程效率高。
占用资源进程>线程>协程
--------------------------------------------
计算密集型：占用大量CPU。用多进程
IO密集型：需要网络功能，大部分时间在等待数据的到来。用多线程、协成
--------------------------------------------
目前的协程框架一般都是设计成 1:N 模式。所谓 1:N 就是一个线程作为一个容器里面放置多个协程。 
那么谁来适时的切换这些协程？答案是有协程自己主动让出CPU，也就是每个协程池里面有一个调度器， 这个调度器是被动调度的。意思就是他不会主动调度。而且当一个协程发现自己执行不下去了(比如异步等待网络的数据回来，但是当前还没有数据到)， 这个时候就可以由这个协程通知调度器，这个时候执行到调度器的代码，调度器根据事先设计好的调度算法找到当前最需要CPU的协程。 切换这个协程的CPU上下文把CPU的运行权交个这个协程，直到这个协程出现执行不下去需要等等的情况，或者它调用主动让出CPU的API之类，触发下一次调度。
假设这个线程中有一个协程是CPU密集型的他没有IO操作， 也就是自己不会主动触发调度器调度的过程，那么就会出现其他协程得不到执行的情况， 所以这种情况下需要程序员自己避免。        
        
在IO密集型的程序中由于IO操作远远慢于CPU的操作，所以往往需要CPU去等IO操作。 同步IO下系统需要切换线程，让操作系统可以在IO过程中执行其他的东西。 这样虽然代码是符合人类的思维习惯但是由于大量的线程切换带来了大量的性能的浪费，尤其是IO密集型的程序。所以人们发明了异步IO。就是当数据到达的时候触发我的回调。
       
进程、线程调用顺序由系统决定
协程调用 顺序由自己决定    
```

生成器和迭代器
生成器实现协成

```python
协程底层是用生成器实现的
# cpu只是切换执行的函数调用就完成了任务。
import time

def A():
    while True:
        print("----A---")
        yield
        time.sleep(0.5)

def B(c):
    while True:
        print("----B---")
        c.next()
        time.sleep(0.5)

if __name__=='__main__':
    a = A()
    B(a)
    
运行结果    
--B--
--A--
--B--
--A--
--B--
--A--
```

greenlet

```python
为了更好使用协程来完成多任务，python中的greenlet模块对其封装，从而使得切换任务变的更加简单
安装greenlet模块:sudo pip install greenlet

#coding=utf-8

from greenlet import greenlet
import time

def test1():
    while True:
        print "---A--"
        gr2.switch() #切换到gr2中运行，再切换回来的时候从下面一句开始执行
        time.sleep(0.5)

def test2():
    while True:
        print "---B--"
        gr1.switch() #切换到gr1中运行
        time.sleep(0.5)

gr1 = greenlet(test1)
gr2 = greenlet(test2)

#切换到gr1中运行
gr1.switch()

---A--
---B--
---A--
---B--
---A--
---B--
```

gevent

```python
greenlet已经实现了协程，但是这个还的人工切换，gevent能够自动切换任务的模块。

gevent在执行时遇到耗时操作自动切换执行别的协程，会交出cpu使用，以此类推。当所有的协成都到了耗时操作了没的切换了，那就等耗时操作完成。
系统能自动识别哪些操作属于耗时操作。
没有耗时操作不会自动切换。

安装gevent模块:sudo pip install gevent
--------------------------------------------
这个不会切换任务执行
#coding=utf-8
#用python 2 来执行此程序
import gevent

def f(n):
    for i in range(n):
        print gevent.getcurrent(), i

# 创建一个对象来执行f函数。
g1 = gevent.spawn(f, 5)
g2 = gevent.spawn(f, 5)
g3 = gevent.spawn(f, 5)
# 清理资源
g1.join() 
g2.join()
g3.join()    
运行结果
<Greenlet "Greenlet-0" at 0x7f4447d8f050: f(5)> 0
<Greenlet "Greenlet-0" at 0x7f4447d8f050: f(5)> 1
<Greenlet "Greenlet-0" at 0x7f4447d8f050: f(5)> 2
<Greenlet "Greenlet-0" at 0x7f4447d8f050: f(5)> 3
<Greenlet "Greenlet-0" at 0x7f4447d8f050: f(5)> 4
<Greenlet "Greenlet-1" at 0x7f4447d8f158: f(5)> 0
<Greenlet "Greenlet-1" at 0x7f4447d8f158: f(5)> 1
<Greenlet "Greenlet-1" at 0x7f4447d8f158: f(5)> 2
<Greenlet "Greenlet-1" at 0x7f4447d8f158: f(5)> 3
<Greenlet "Greenlet-1" at 0x7f4447d8f158: f(5)> 4
<Greenlet "Greenlet-2" at 0x7f4447d8f260: f(5)> 0
<Greenlet "Greenlet-2" at 0x7f4447d8f260: f(5)> 1
<Greenlet "Greenlet-2" at 0x7f4447d8f260: f(5)> 2
<Greenlet "Greenlet-2" at 0x7f4447d8f260: f(5)> 3
<Greenlet "Greenlet-2" at 0x7f4447d8f260: f(5)> 4

--------------------------------------------
切换任务执行：在执行到IO操作时，gevent自动切换
import gevent

def f(n):
    for i in range(n):
        print gevent.getcurrent(), i
        # 这是一个耗时操作。
        gevent.sleep(1)

g1 = gevent.spawn(f, 5)
g2 = gevent.spawn(f, 5)
g3 = gevent.spawn(f, 5)
g1.join()
g2.join()
g3.join()
运行结果
<Greenlet "Greenlet-0" at 0x7f9f995da050: f(5)> 0
<Greenlet "Greenlet-1" at 0x7f9f995da158: f(5)> 0
<Greenlet "Greenlet-2" at 0x7f9f995da260: f(5)> 0
<Greenlet "Greenlet-0" at 0x7f9f995da050: f(5)> 1
<Greenlet "Greenlet-1" at 0x7f9f995da158: f(5)> 1
<Greenlet "Greenlet-2" at 0x7f9f995da260: f(5)> 1
<Greenlet "Greenlet-0" at 0x7f9f995da050: f(5)> 2
<Greenlet "Greenlet-2" at 0x7f9f995da260: f(5)> 2
<Greenlet "Greenlet-1" at 0x7f9f995da158: f(5)> 2
<Greenlet "Greenlet-0" at 0x7f9f995da050: f(5)> 3
<Greenlet "Greenlet-1" at 0x7f9f995da158: f(5)> 3
<Greenlet "Greenlet-2" at 0x7f9f995da260: f(5)> 3
<Greenlet "Greenlet-0" at 0x7f9f995da050: f(5)> 4
<Greenlet "Greenlet-2" at 0x7f9f995da260: f(5)> 4
<Greenlet "Greenlet-1" at 0x7f9f995da158: f(5)> 4
--------------------------------------------
gevent版-TCP服务器，单进程单线程完成并发
在执行到IO操作时，gevent自动切换
import sys
import time
import gevent

from gevent import socket,monkey
# 写在最前面
# gevent完成并发服务器需要写这句话，他在运行过程中修改python源代码。
monkey.patch_all()

def handle_request(conn):
    while True:
        # recv等待接收消息，耗时操作，不会阻塞，而是切换协程
        data = conn.recv(1024)
        if not data:
            conn.close()
            break
        print("recv:", data)
        conn.send(data)


def server(port):
    s = socket.socket()
    s.bind(('', port))
    s.listen(5)
    while True:
        # 如果时第一次执行到这里还没有创建协成，没法切换，就一直等，直到有客户端来连接
        # 当有协程了，accept等待客户端连接，耗时操作，不会阻塞而是切换协成执行。
        cli, addr = s.accept()
        # 创建协程
        gevent.spawn(handle_request, cli)

if __name__ == '__main__':
    server(7788)

# 使用gevent的socket，不能使用socket模块的socket。gevent进行了重写。
# 携程切换会从上次切换的位置继续执行。


```

# 参考

```
https://juejin.cn/post/7050773195745411085
https://juejin.cn/post/7050280727207739406#heading-18

I/O 多路复用
    总结: https://www.cnblogs.com/Anker/p/3265058.html
    select: http://www.cnblogs.com/Anker/archive/2013/08/14/3258674.html
    poll: http://www.cnblogs.com/Anker/archive/2013/08/15/3261006.html
    epoll: http://www.cnblogs.com/Anker/archive/2013/08/17/3263780.html
```



