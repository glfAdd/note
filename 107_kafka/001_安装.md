## linux

##### 安装

```
# 官网
http://kafka.apache.org/downloads.html
# 源代码, 需要编译
kafka-2.8.0-src.tgz 
# 编译好, 可以直接使用
kafka_2.13-2.8.0.tgz


1. 下载
wget https://mirrors.bfsu.edu.cn/apache/kafka/2.8.0/kafka_2.13-2.8.0.tgz

2. 启动kafka需要先启动zookeeper

3. 启动kafka
nohup bin/kafka-server-start.sh config/server.properties > /dev/null 2>&1 &
```

- 通过zookeeper检测是否启动成功

```
1. zookeeper 客户端连接 zookeeper
./zkCli.sh -server localhost:2181

2. 查看是否有 kafka 创建的数据
[zk: localhost:2181(CONNECTED) 0] ls /brokers
[ids, seqid, topics]

3. 如果显示有值, 则表示 broker.id 为 0 的broker已经加入了kafka集群中
[zk: localhost:2181(CONNECTED) 1] ls /brokers/ids 
[0]
```

- 配置文件 config/server.properties

```
broker.id=0
log.dirs=kafka-logs
zookeeper.connect=localhost:2181
```

##### 配置文件 config/server.properties

```
############################# Server Basics #############################
# 在一个kafka集群中，每一台kafka都要有唯一的broker.id, 且必须为整数
broker.id=10

############################# Socket Server Settings #############################
# 默认处理网络请求的线程个数 3个
num.network.threads=3
# 执行磁盘IO操作的默认线程个数 8
num.io.threads=8
# socket服务使用的进行发送数据的缓冲区大小，默认100kb
socket.send.buffer.bytes=102400
# socket服务使用的进行接受数据的缓冲区大小，默认100kb
socket.receive.buffer.bytes=102400
# socket服务所能够接受的最大的请求量，防止出现OOM(Out of memory)内存溢出，默认值为：100m
socket.request.max.bytes=104857600

############################# Log Basics （数据相关部分，kafka的数据称为log）#############################
# 一个用逗号分隔的目录列表，用于存储kafka接受到的数据
log.dirs=/home/uplooking/data/kafka
# 每一个topic所对应的log的partition分区数目，默认1个
num.partitions=1
# 每一个数据目录用于在启动kafka时恢复数据和在关闭时刷新数据的线程个数。如果kafka数据存储在磁盘阵列中建议此值可以调整更大
num.recovery.threads.per.data.dir=1

############################# Log Flush Policy （数据刷新策略）#############################
# 基于消息条数和时间间隔数来制定数据刷新策略, 可以只设置一个或两个都配置
# 消息刷新到磁盘中的消息条数阈值
#log.flush.interval.messages=10000

# 消息刷新到磁盘生成一个log数据文件的时间间隔
#log.flush.interval.ms=1000

############################# Log Retention Policy（数据保留策略） #############################
# 控制数据片段的清理，只要满足其中一个策略, 分片就会被删除
# 基于时间的策略，删除日志数据的时间，默认保存7天
log.retention.hours=168

# 基于大小的策略，1G
#log.retention.bytes=1073741824
# 数据大小分片策略
log.segment.bytes=1073741824
# 每隔多长时间检测数据是否达到删除条件
log.retention.check.interval.ms=300000

############################# Zookeeper #############################
zookeeper.connection.timeout.ms=6000
```

##### 图形工具 kafka manager

- 系统要求

```
Kafka 需要新版
Java 11+
```

- sdkman安装需要的java版本

```
sdk install java 11.0.11-open
```

- 安装

```
github地址: https://github.com/yahoo/kafka-manager

release 二进制文件下载地址: https://github.com/yahoo/CMAK/releases
```

- 备份并修改配置 conf/application.conf 文件

```
# kafka-manager.zkhosts 和 cmak.zkhosts 改为实际的地址
kafka-manager.zkhosts="localhost:2181"
kafka-manager.zkhosts=${?ZK_HOSTS}
cmak.zkhosts="localhost:2181"
cmak.zkhosts=${?ZK_HOSTS}


# 多个中间用逗号分隔
kafka-manager.zkhosts="node21:2181,node22:2181,node23:2181"
```

- 启动

```
bin/cmak -java-home /home/glfadd/.sdkman/candidates/java/11.0.11-open -Dconfig.file=conf/application.conf -Dhttp.port=9091 

参考
bin/cmak -Dconfig.file=conf/application.conf -java-home $JAVA_HOME11 -Dhttp.port=9091


参数:
    -java-home 指定java的路径
    -Dconfig.file 指定配置文件
    -Dhttp.port 指定端口


web 页面
http://127.0.0.1:9091/
```

- supervisor

```
[program:cmak]
directory=/opt/cmak-3.0.0.5
command=/opt/cmak-3.0.0.5/bin/cmak -java-home /home/glfadd/.sdkman/candidates/java/11.0.11-open -Dconfig.file=conf/application.conf -Dhttp.port=19082
autostart=false
autorestart=false
user=glfadd
log_stdout=true
log_stderr=true
redirect_stderr = true
stdout_logfile_maxbytes = 20MB
stdout_logfile_backups = 20     
stdout_logfile = /opt/logs/supervisord_cmak.log
```

- 使用

```
https://www.jianshu.com/p/6a592d558812
https://blog.csdn.net/wsdc0521/article/details/105846074

```



IDE

